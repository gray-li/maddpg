{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\t\nUnity brain name: TennisBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 8\n        Number of stacked Vector Observation: 3\n        Vector Action space type: continuous\n        Vector Action space size (per agent): 2\n        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name = 'Tennis.app')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of agents: 2\nSize of each action: 2\nThere are 2 agents. Each observes a state with length: 24\nThe state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.         -6.65278625 -1.5\n -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "The next cell shows the performance of a random agent and the commands needed to interact with the Unity environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Solving the Environment\n",
    "\n",
    "Next we implement Proximal Policy Optimization (PPO) to solve the environment.\n",
    "\n",
    "When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "\n",
    "First, we import additional packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this environment, we'll be using DDPG and a couple of variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DDPGModel, MADDPGModel, MAD3PGModel\n",
    "from DDPG_agent import DDPGAgent\n",
    "from MADDPG_agent import MADDPGAgent, MAD3PGAgent\n",
    "from exploration_noise import OrnsteinUhlenbeckProcess as OUNoise, GaussianProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_agent(env, multi_agent, n_episodes = 10000, n_agents = 2,\n",
    "                      evaluation_window = 100, verbose = 100, solved_score = 1.0):\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    for i in range(n_agents):\n",
    "        multi_agent.online_networks[i].to(device)\n",
    "        multi_agent.target_networks[i].to(device)\n",
    "    \n",
    "    multi_agent.reset_current_step()\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        total_rewards = np.zeros(n_agents)\n",
    "    \n",
    "        env_info = env.reset(train_mode = True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        \n",
    "        while True:\n",
    "            action = multi_agent.action(state)\n",
    "            \n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations\n",
    "            reward = np.array(env_info.rewards)\n",
    "            done = np.array(env_info.local_done)\n",
    "            \n",
    "            total_rewards += reward\n",
    "            \n",
    "            multi_agent.update(state, action, reward, next_state, done)\n",
    "\n",
    "            if np.any(done):\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        scores.append(np.max(total_rewards))\n",
    "        \n",
    "        average_score = np.mean(scores[-min(evaluation_window, i + 1):])\n",
    "        current_score = scores[-1]\n",
    "        print('\\rEpisode {} | Score: {:.2f} | Average Score: {:.2f}'.format(i + 1, current_score, average_score), end = '')\n",
    "        sys.stdout.flush()\n",
    "        if (i + 1) % verbose == 0:\n",
    "            print('')\n",
    "            \n",
    "        if average_score > solved_score:\n",
    "            print('\\nEnvironment solved in {} episodes'.format(i + 1))\n",
    "            break\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantScheduler():\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.step = 0\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.step += 1\n",
    "        value = self.value\n",
    "        \n",
    "        return value\n",
    "        \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "class ExponentialScheduler():\n",
    "    def __init__(self, start_value, end_value, rate):\n",
    "        self.start_value = start_value\n",
    "        self.end_value = end_value\n",
    "        self.rate = rate\n",
    "        self.value = start_value\n",
    "        self.step = 0\n",
    "        \n",
    "        if end_value > start_value:\n",
    "            assert rate > 1, 'Rate must be greater than one when the ending value is greater than the starting value'\n",
    "            self.bound = min\n",
    "        elif end_value < start_value:\n",
    "            assert rate < 1, 'Rate must be less than one when the ending value is less than the starting value'\n",
    "            self.bound = max\n",
    "        else:\n",
    "            raise ValueError('Start and end value cannot be the same')\n",
    "\n",
    "    def __call__(self):\n",
    "        self.step += 1\n",
    "        value = self.value\n",
    "        \n",
    "        self.value = self.bound(self.value * self.rate, self.end_value)\n",
    "        \n",
    "        return value\n",
    "        \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "        self.value = self.start_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MADDPG Agents with Gaussian Noise\n",
      "Episode 100 | Score: 0.00 | Average Score: 0.03\n",
      "Episode 200 | Score: 0.00 | Average Score: 0.01\n",
      "Episode 300 | Score: 0.00 | Average Score: 0.01\n",
      "Episode 400 | Score: 0.00 | Average Score: 0.02\n",
      "Episode 420 | Score: 0.10 | Average Score: 0.02"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-94125115c019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m scores = train_multi_agent(env, MADDPG_agents, n_episodes = 5000, n_agents = 2,\n\u001b[0;32m---> 18\u001b[0;31m                             evaluation_window = 100, verbose = 100, solved_score = 1.0)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mmaddpg_gn_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-25a37e49b2a8>\u001b[0m in \u001b[0;36mtrain_multi_agent\u001b[0;34m(env, multi_agent, n_episodes, n_agents, evaluation_window, verbose, solved_score)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mmulti_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/maddpg/MADDPG_agent.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, state, action, reward, next_state, terminal)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('MADDPG Agents with Gaussian Noise')\n",
    "maddpg_gn_scores = []\n",
    "\n",
    "model_fn = lambda: MADDPGModel(2, 24, 2, critic_hidden_layers = (256, 128), actor_hidden_layers = (256, 128))\n",
    "\n",
    "MADDPG_agents = MADDPGAgent(2, model_fn,\n",
    "                            exploration_noise_fn = lambda: GaussianProcess((2, ), \n",
    "                                                                            ExponentialScheduler(3.0, .1, .99995)),\n",
    "                            batch_size = 256, \n",
    "                            replay_start = 1000,\n",
    "                            tau = 1e-3,\n",
    "                            actor_learning_rate = 1e-4,\n",
    "                            critic_learning_rate = 1e-3,\n",
    "                            clip_gradients = 1.0,\n",
    "                            share_weights = False)\n",
    "\n",
    "scores = train_multi_agent(env, MADDPG_agents, n_episodes = 5000, n_agents = 2,\n",
    "                            evaluation_window = 100, verbose = 100, solved_score = 0.5)\n",
    "maddpg_gn_scores.append(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(MAD3PG_agents.online_networks[0].state_dict(), 'agent_1_online_networks.pt')\n",
    "torch.save(MAD3PG_agents.online_networks[1].state_dict(), 'agent_2_online_networks.pt')\n",
    "torch.save(MAD3PG_agents.target_networks[0].state_dict(), 'agent_1_target_networks.pt')\n",
    "torch.save(MAD3PG_agents.target_networks[1].state_dict(), 'agent_2_target_networks.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}